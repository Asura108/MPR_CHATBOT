{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cdf6ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c6ce88",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "    MPR PROJECT : CHATBOT\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d78db8",
   "metadata": {},
   "source": [
    "#                          MODEL USED IN THE PROJECT : GPT - 3                                                \n",
    "\n",
    "\n",
    "GPT-3 (Generative Pre-training Transformer 3) is a state-of-the-art language processing model developed by OpenAI. \n",
    "It is trained on a massive amount of data and is capable of generating human-like text, translating languages, \n",
    "answering questions, and performing a wide range of other language tasks.\n",
    "\n",
    "-------------- x ------------- x ------------- x --------------------------- x----------------------x----------------x\n",
    "\n",
    "In this notebook, we will explore some of the algorithms that are used in the GPT-3 model and how they are used\n",
    "to perform various language tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd634c5",
   "metadata": {},
   "source": [
    "## Transformer Architecture\n",
    "\n",
    "One of the key algorithms used in GPT-3 is the transformer architecture, which is a type of neural network that is particularly well-suited for processing sequential data such as text. The transformer architecture was introduced in the paper \"Attention is All You Need\" (https://arxiv.org/abs/1706.03762) and has since become widely used in natural language processing and other fields.\n",
    "\n",
    "The transformer architecture consists of a series of \"layers\" that process the input data. Each layer consists of two parts: a self-attention mechanism and a feedforward network. The self-attention mechanism allows the model to \"attend\" to different parts of the input data at each layer, while the feedforward network transforms the input data using a series of linear transformations and nonlinear activations.\n",
    "\n",
    "The transformer architecture has several key advantages over previous models such as recurrent neural networks (RNNs). It is much faster to train and can parallelize the processing of the input data, which makes it well-suited for large-scale tasks such as language translation. It also has a more flexible architecture that allows it to model longer-range dependencies in the data.\n",
    "\n",
    "## Pre-training and Fine-tuning\n",
    "\n",
    "GPT-3 is a pre-trained model, which means that it was trained on a large dataset in advance and can then be fine-tuned for specific tasks. Pre-training allows the model to learn general-purpose language representations that can be used for a wide range of tasks, while fine-tuning allows the model to adapt to the specific characteristics of a particular task.\n",
    "\n",
    "For example, GPT-3 can be fine-tuned for tasks such as language translation, question answering, and text generation by providing it with additional training data and adjusting the model's parameters to optimize performance on the specific task.\n",
    "\n",
    "## Examples\n",
    "\n",
    "Here are a few examples of how the algorithms used in GPT-3 can be used to perform various language tasks:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c826384f",
   "metadata": {},
   "source": [
    "### ▶Language Translation\n",
    "\n",
    "To translate a sentence from one language to another using GPT-3, we can fine-tune the model on a large dataset of translated sentences and then provide it with a new sentence to translate. The model will use its knowledge of the two languages and the structure of translations to generate a translation of the input sentence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab26a374",
   "metadata": {},
   "source": [
    "### Example of Language translation via code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d28df33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el gato está en la mesa\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_engine = \"text-ada-001\"\n",
    "prompt = \"Translate this sentence from English to Spanish: 'The cat is on the table.'\"\n",
    "completions = openai.Completion.create(\n",
    "    engine=model_engine,\n",
    "    prompt=prompt,\n",
    "    max_tokens=1000,\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "translation = completions.choices[0].text\n",
    "print(translation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9514bc",
   "metadata": {},
   "source": [
    "### ▶Question Answering\n",
    "\n",
    "To answer a question using GPT-3, we can fine-tune the model on a large dataset of questions and answers and then provide it with a new question to answer. The model will use its understanding of the topic and the structure of questions and answers to generate a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda5ed5c",
   "metadata": {},
   "source": [
    "### Example of Question answering via code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10b5ce93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The American Civil War was fought between the Confederate States of America (CSA) and the Union states. The CSA was created in 1861, and in 1865 they became a single country. The Union states were a part of the European Union, which included the United States. The CSA was bad for business, and the Union states didn't want to be associated with the United States.\n"
     ]
    }
   ],
   "source": [
    "model_engine = \"text-ada-001\"\n",
    "prom = \"What was the main cause of the American Civil War?\"\n",
    "completions = openai.Completion.create(\n",
    "    engine=model_engine,\n",
    "    prompt=prom,\n",
    "    max_tokens=1000,\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=0.5,\n",
    ")\n",
    "answer = completions.choices[0].text\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e1ae09",
   "metadata": {},
   "source": [
    "### ▶Text Generation\n",
    "\n",
    "To generate text using GPT-3, we can provide the model with a prompt or seed text and ask it to generate additional text based on the prompt. The model will use its understanding of language structure and content to generate text that is coherent and follows the style and theme of the prompt.\n",
    "\n",
    "For example, we could provide GPT-3 with the prompt \"In the year 2045, humans have colonized Mars and established a thriving society on the planet. However, not everything is perfect. Describe some of the challenges that the Martian colonists face.\" The model might generate text such as:\n",
    "\n",
    "\"One of the major challenges faced by the Martian colonists is the harsh environmental conditions on the planet. The thin atmosphere and extreme temperature fluctuations make it difficult for humans to survive without specialized equipment. Another challenge is the limited resources available on Mars, as the colonists must rely on supplies brought from Earth or materials that can be extracted from the Martian soil. Additionally, the isolation of living on another planet can be mentally taxing for some individuals, as they are cut off from their friends and family on Earth.\"\n",
    "\n",
    "As you can see, the text generated by GPT-3 is coherent and follows the theme and style of the prompt, demonstrating its understanding of language and its ability to generate text that is similar to human-written text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea888eb",
   "metadata": {},
   "source": [
    "### Example of text generation via code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68038802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Artificial Intelligence (AI) is a rapidly growing technology that is being used in a variety of different ways. AI is a computer system that is capable of performing tasks that would normally require human intelligence. AI can be used to automate mundane tasks, provide insights from data, and even help with decision making. AI has the potential to revolutionize the way we do things and has already been used to create robots that can do a variety of tasks. AI can also be used to create more efficient and accurate systems to help businesses run more efficiently. AI is becoming increasingly important in a number of industries and is set to become an integral part of our lives.\n"
     ]
    }
   ],
   "source": [
    "model_engine = \"text-davinci-003\"\n",
    "p = \"Generate a paragraph about artificial intelligence.\"\n",
    "completions = openai.Completion.create(\n",
    "    engine=model_engine,\n",
    "    prompt=p,\n",
    "    max_tokens=1000,\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=0.5,\n",
    ")\n",
    "paragraph = completions.choices[0].text\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076f87b7",
   "metadata": {},
   "source": [
    "# Parameters used in GPT-3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df323d1",
   "metadata": {},
   "source": [
    "Some of the key parameters in GPT-3 and demonstrate how they can be used through code examples.\n",
    "\n",
    "### 1) Prompt\n",
    "\n",
    "The prompt is the input text that is provided to GPT-3. It can be a single word, a phrase, a sentence, or a longer passage. The prompt serves as a starting point for the model to generate additional text.\n",
    "\n",
    "Here is an example of using a simple prompt to generate a single word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "186b2783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Druid\n"
     ]
    }
   ],
   "source": [
    "model_engine = \"text-ada-001\"\n",
    "pt = \"Generate a word.\"\n",
    "\n",
    "completions = openai.Completion.create(\n",
    "    engine=model_engine,\n",
    "    prompt=pt,\n",
    "    max_tokens=10,\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=0.7,\n",
    ")\n",
    "\n",
    "word = completions.choices[0].text\n",
    "\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f470fc5",
   "metadata": {},
   "source": [
    "### 2) Max Tokens\n",
    "\n",
    "The max_tokens parameter specifies the maximum number of tokens (i.e. words and punctuation) that the model is allowed to generate. This can be useful for controlling the length of the generated text or limiting the amount of computational resources used.\n",
    "\n",
    "Here is an example of using max_tokens to generate a short sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c88a370b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I have a lot of energy.\n"
     ]
    }
   ],
   "source": [
    "model_engine = \"text-ada-001\"\n",
    "prompt = \"Generate a sentence.\"\n",
    "completions = openai.Completion.create(\n",
    "    engine=model_engine,\n",
    "    prompt=prompt,\n",
    "    max_tokens=10,\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=0.7,\n",
    ")\n",
    "sentence = completions.choices[0].text\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dafeba8",
   "metadata": {},
   "source": [
    "### Using the same prompt but with less tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8231e7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I can't\n"
     ]
    }
   ],
   "source": [
    "model_engine = \"text-ada-001\"\n",
    "prompt = \"Generate a sentence.\"\n",
    "completions = openai.Completion.create(\n",
    "    engine=model_engine,\n",
    "    prompt=prompt,\n",
    "    max_tokens=5,\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=0.7,\n",
    ")\n",
    "sentence = completions.choices[0].text\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a099a6fb",
   "metadata": {},
   "source": [
    "### 3) N\n",
    "\n",
    "The n parameter specifies the number of completions to generate for a given prompt. This can be useful for generating multiple variations of the same prompt or for generating a large number of completions in a batch.\n",
    "\n",
    "Here is an example of using n to generate multiple completions for a single prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b401054c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n\\nThe sun is the largest object in space.', '\\n\\nI have a lot of energy.', \"\\n\\nI'm not sure if I want to do this.\", \"\\n\\nI have a lot of friends, but I don't talk to them much.\", '\\n\\nShe loved spending time with her family and friends, but she also knew that they could be difficult at times.']\n"
     ]
    }
   ],
   "source": [
    "model_engine = \"text-ada-001\"\n",
    "prompt = \"Generate a sentence.\"\n",
    "completions = openai.Completion.create(\n",
    "    engine=model_engine,\n",
    "    prompt=prompt,\n",
    "    max_tokens=1000,\n",
    "    n=5,\n",
    "    stop=None,\n",
    "    temperature=0.7,\n",
    ")\n",
    "sentences = [choice.text for choice in completions.choices]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a48e324",
   "metadata": {},
   "source": [
    "Now running the same prompt with 2 tokens, we will get only 2 answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae531416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n\\nThe sun is the largest star in the sky.', \"\\n\\nI can't believe it's only the weekend.\"]\n"
     ]
    }
   ],
   "source": [
    "model_engine = \"text-ada-001\"\n",
    "prompt = \"Generate a sentence.\"\n",
    "completions = openai.Completion.create(\n",
    "    engine=model_engine,\n",
    "    prompt=prompt,\n",
    "    max_tokens=1000,\n",
    "    n=2,\n",
    "    stop=None,\n",
    "    temperature=0.7,\n",
    ")\n",
    "sentences = [choice.text for choice in completions.choices]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaabac16",
   "metadata": {},
   "source": [
    "### 4) Stop\n",
    "\n",
    "The stop parameter specifies a string that, when encountered by the model, will cause it to stop generating additional text. This can be useful for generating text that meets certain criteria or for controlling the length of the generated text.\n",
    "\n",
    "Here is an example of using stop to generate a paragraph that ends with a specific word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74189972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The morning sun shone brightly through \n"
     ]
    }
   ],
   "source": [
    "model_engine= \"text-davinci-003\"\n",
    "prompt = \"Generate a paragraph that ends with the word 'the'.\"\n",
    "completions = openai.Completion.create(\n",
    "    engine=model_engine,\n",
    "    prompt=prompt,\n",
    "    max_tokens=1000,\n",
    "    n=1,\n",
    "    stop=\"the\",\n",
    "    temperature=0.7,\n",
    ")\n",
    "paragraph = completions.choices[0].text\n",
    "print(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0df26ca",
   "metadata": {},
   "source": [
    "### 5) Temperature\n",
    "\n",
    "The temperature parameter controls the randomness of the generated text. A higher temperature will result in more random and diverse text, while a lower temperature will result in more predictable and repetitive text.\n",
    "\n",
    "Here is an example of using temperature to generate text with different levels of randomness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2017157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature 0.7: \n",
      "\n",
      "The sun was shining brightly on the calm lake.\n",
      "Temperature 0.5: \n",
      "\n",
      "The sun was shining brightly on the warm summer day.\n",
      "Temperature 0.2: \n",
      "\n",
      "The sun was shining brightly on the beautiful day.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Generate a sentence.\"\n",
    "\n",
    "completions = openai.Completion.create(\n",
    "    engine=model_engine,\n",
    "    prompt=prompt,\n",
    "    max_tokens=1000,\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=0.7,\n",
    ")\n",
    "sentence1 = completions.choices[0].text\n",
    "\n",
    "#now setting temperature as 0.5 , i.e decreasing temperature\n",
    "\n",
    "completions = openai.Completion.create(\n",
    "    engine=model_engine,\n",
    "    prompt=prompt,\n",
    "    max_tokens=1000,\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=0.5,\n",
    ")\n",
    "sentence2 = completions.choices[0].text\n",
    "\n",
    "#decreasing temperature further\n",
    "\n",
    "completions = openai.Completion.create(\n",
    "    engine=model_engine,\n",
    "    prompt=prompt,\n",
    "    max_tokens=1000,\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=0.2,\n",
    ")\n",
    "sentence3 = completions.choices[0].text\n",
    "\n",
    "print(f\"Temperature 0.7: {sentence1}\")\n",
    "print(f\"Temperature 0.5: {sentence2}\")\n",
    "print(f\"Temperature 0.2: {sentence3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385e194f",
   "metadata": {},
   "source": [
    "# Comparison amongst the four GPT-3 models.\n",
    "\n",
    "GPT-3 uses a deep neural network with 175 billion parameters to generate human-like text. GPT-3 can be fine-tuned for a wide range of language tasks, such as language translation, text summarization, question answering, and text generation.\n",
    "\n",
    "The four GPT-3 models: Davinci, Ada, Babbage and Curie are all different variations of GPT-3, but with different capabilities and limitations.\n",
    "\n",
    "▶Davinci is the largest GPT-3 model with 175 billion parameters, the most computationally expensive and therefore the most powerful one. It can perform a wide range of language tasks with high accuracy and can generate text that is almost indistinguishable from human-written text.\n",
    "\n",
    "▶Ada is a smaller version of Davinci with fewer parameters (60 billion) and lower computational requirements. It can still perform a wide range of language tasks, but its performance may not be as high as Davinci's.\n",
    "\n",
    "▶Babbage is an even smaller version of GPT-3 with 40 billion parameters, lower computational requirements and its focus is mainly on the logical and analytical reasoning, it is more specialized in language understanding and reasoning .\n",
    "\n",
    "▶Curie is the smallest GPT-3 model with only 4 billion parameters and it's focused on understanding and responding to specific questions, it is more specialized in answering questions.\n",
    "\n",
    "Here is a sample code which shows the similarities and differences between the four GPT-3 models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96813d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story generated by text-davinci-002\n",
      "\n",
      "\n",
      "He is a superhero who fights crime in Gotham City. He has no superpowers, but is an expert in martial arts and has a lot of cool gadgets. He is also a billionaire.\n",
      "Story generated by text-ada-001\n",
      "\n",
      "\n",
      "batman is a very confident person who is not afraid to be public. he is also a very friendly person and would be interested in everyone he meets.\n",
      "Story generated by text-babbage-001\n",
      "\n",
      "\n",
      "Batman is a vigilante who uses his skills as a detective and batman suit to fight crime. He is a strong and determined individual who is always looking out for the best interests of those he cares for.\n",
      "Story generated by text-curie-001\n",
      "\n",
      "\n",
      "Batman is a superhero who uses his intelligence, strength, and athleticism to fight crime. He is known for wearing a costume and using a bat to fight against criminals.\n"
     ]
    }
   ],
   "source": [
    "models = [\"text-davinci-002\", \"text-ada-001\", \"text-babbage-001\", \"text-curie-001\"]\n",
    "prompt = \"describe the character of batman in 30 tokens or less\"\n",
    "\n",
    "for model in models:\n",
    "    completions = openai.Completion.create(\n",
    "        engine=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=50,\n",
    "        n=1,\n",
    "        stop=None,\n",
    "        temperature=0.1,\n",
    "    )\n",
    "    message = completions.choices[0].text\n",
    "    print(f\"Story generated by {model}\")\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f82e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
